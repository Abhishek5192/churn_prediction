{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, udf, isnan, count, when, desc, sort_array, asc, avg, lag, floor\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType, DateType\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession.builder.appName(\"spark_capstone\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "Clean your dataset, checking for invalid or missing data. For example, records without userids or sessionids. In this workspace, the filename is `mini_sparkify_event_data.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.json(\"mini_sparkify_event_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8346 0\n"
     ]
    }
   ],
   "source": [
    "print(data.select(\"userId\").where(data.userId ==\"\").count(), \n",
    "      data.select(\"sessionId\").where(data.sessionId ==\"\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286500 228108 286500\n"
     ]
    }
   ],
   "source": [
    "print(data.count(), data.dropna(how='any').count(), data.dropna(how='any', subset=['userId', 'sessionId']).count())\n",
    "#no NA values in these two columns. the rest of the columns could have 'null' values but depending on what user is\n",
    "#doing we do not always need to have every field filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.where((data.userId != \"\") | (data.sessionId != \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278154"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                page|\n",
      "+--------------------+\n",
      "|               About|\n",
      "|          Add Friend|\n",
      "|     Add to Playlist|\n",
      "|              Cancel|\n",
      "|Cancellation Conf...|\n",
      "|           Downgrade|\n",
      "|               Error|\n",
      "|                Help|\n",
      "|                Home|\n",
      "|              Logout|\n",
      "|            NextSong|\n",
      "|         Roll Advert|\n",
      "|       Save Settings|\n",
      "|            Settings|\n",
      "|    Submit Downgrade|\n",
      "|      Submit Upgrade|\n",
      "|         Thumbs Down|\n",
      "|           Thumbs Up|\n",
      "|             Upgrade|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"page\").dropDuplicates().sort(\"page\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artist unique values: 17656\n",
      "+--------------------+\n",
      "|              artist|\n",
      "+--------------------+\n",
      "|                null|\n",
      "|                 !!!|\n",
      "|        & And Oceans|\n",
      "|'N Sync/Phil Collins|\n",
      "|        'Til Tuesday|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Auth unique values: 2\n",
      "+---------+\n",
      "|     auth|\n",
      "+---------+\n",
      "|Cancelled|\n",
      "|Logged In|\n",
      "+---------+\n",
      "\n",
      "Firstname unique values: 189\n",
      "+---------+\n",
      "|firstName|\n",
      "+---------+\n",
      "| Adelaida|\n",
      "|   Adrian|\n",
      "|  Adriana|\n",
      "|   Adriel|\n",
      "|  Ainsley|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Gender unique values: 2\n",
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|     F|\n",
      "|     M|\n",
      "+------+\n",
      "\n",
      "Iteminsession unique values: 1311\n",
      "+-------------+\n",
      "|itemInSession|\n",
      "+-------------+\n",
      "|            0|\n",
      "|            1|\n",
      "|            2|\n",
      "|            3|\n",
      "|            4|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Lastname unique values: 173\n",
      "+---------+\n",
      "| lastName|\n",
      "+---------+\n",
      "|    Adams|\n",
      "|  Aguilar|\n",
      "|Alexander|\n",
      "|    Allen|\n",
      "| Atkinson|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Length unique values: 14866\n",
      "+-------+\n",
      "| length|\n",
      "+-------+\n",
      "|   null|\n",
      "|0.78322|\n",
      "|2.16771|\n",
      "|4.04853|\n",
      "| 4.8322|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Level unique values: 2\n",
      "+-----+\n",
      "|level|\n",
      "+-----+\n",
      "| free|\n",
      "| paid|\n",
      "+-----+\n",
      "\n",
      "Location unique values: 114\n",
      "+--------------------+\n",
      "|            location|\n",
      "+--------------------+\n",
      "|          Albany, OR|\n",
      "|Albany-Schenectad...|\n",
      "|      Alexandria, LA|\n",
      "|Allentown-Bethleh...|\n",
      "|       Anchorage, AK|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Method unique values: 2\n",
      "+------+\n",
      "|method|\n",
      "+------+\n",
      "|   GET|\n",
      "|   PUT|\n",
      "+------+\n",
      "\n",
      "Page unique values: 19\n",
      "+--------------------+\n",
      "|                page|\n",
      "+--------------------+\n",
      "|               About|\n",
      "|          Add Friend|\n",
      "|     Add to Playlist|\n",
      "|              Cancel|\n",
      "|Cancellation Conf...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Registration unique values: 225\n",
      "+-------------+\n",
      "| registration|\n",
      "+-------------+\n",
      "|1521380675000|\n",
      "|1526739206000|\n",
      "|1526838391000|\n",
      "|1528403713000|\n",
      "|1528560242000|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Sessionid unique values: 2312\n",
      "+---------+\n",
      "|sessionId|\n",
      "+---------+\n",
      "|        1|\n",
      "|        2|\n",
      "|        3|\n",
      "|        4|\n",
      "|        5|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Song unique values: 58481\n",
      "+--------------------+\n",
      "|                song|\n",
      "+--------------------+\n",
      "|                null|\n",
      "|\u001c",
      "ÃÂg ÃÂtti Gr...|\n",
      "| I Will Not Reap ...|\n",
      "|              !@*$%#|\n",
      "|#!*@ You Tonight ...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Status unique values: 3\n",
      "+------+\n",
      "|status|\n",
      "+------+\n",
      "|   200|\n",
      "|   307|\n",
      "|   404|\n",
      "+------+\n",
      "\n",
      "Ts unique values: 269770\n",
      "+-------------+\n",
      "|           ts|\n",
      "+-------------+\n",
      "|1538352117000|\n",
      "|1538352180000|\n",
      "|1538352394000|\n",
      "|1538352416000|\n",
      "|1538352676000|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Useragent unique values: 56\n",
      "+--------------------+\n",
      "|           userAgent|\n",
      "+--------------------+\n",
      "|\"Mozilla/5.0 (Mac...|\n",
      "|\"Mozilla/5.0 (Mac...|\n",
      "|\"Mozilla/5.0 (Mac...|\n",
      "|\"Mozilla/5.0 (Mac...|\n",
      "|\"Mozilla/5.0 (Mac...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Userid unique values: 225\n",
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "|    10|\n",
      "|   100|\n",
      "|100001|\n",
      "|100002|\n",
      "|100003|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    print(f'{column.title()} unique values: {df.select(column).dropDuplicates().count()}')\n",
    "    df.select(column).dropDuplicates().sort(column).show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = udf(lambda x: int(x==\"Cancellation Confirmation\"), IntegerType())\n",
    "downgrade_churn = udf(lambda x: int(x==\"Submit Downgrade\"), IntegerType())\n",
    "\n",
    "df = df.withColumn(\"downgraded\", downgrade_churn(\"page\")).withColumn(\"cancelled\", churn(\"page\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowval = Window.partitionBy(\"userId\").orderBy(desc(\"ts\")).rangeBetween(Window.unboundedPreceding, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"churn_phase\", Fsum(\"cancelled\").over(windowval))\\\n",
    "    .withColumn(\"downgrade_phase\", Fsum(\"downgraded\").over(windowval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------+-----+-----------+-------------+\n",
      "|firstName|cancelled|downgraded|level|churn_phase|           ts|\n",
      "+---------+---------+----------+-----+-----------+-------------+\n",
      "|    Mason|        0|         0| free|          1|1539317144000|\n",
      "|    Mason|        0|         0| free|          1|1539317481000|\n",
      "|    Mason|        0|         0| free|          1|1539317711000|\n",
      "|    Mason|        0|         0| free|          1|1539317914000|\n",
      "|    Mason|        0|         0| free|          1|1539318124000|\n",
      "|    Mason|        0|         0| free|          1|1539318515000|\n",
      "|    Mason|        0|         0| free|          1|1539318728000|\n",
      "|    Mason|        0|         0| free|          1|1539318906000|\n",
      "|    Mason|        0|         0| free|          1|1539318917000|\n",
      "|    Mason|        0|         0| free|          1|1539318918000|\n",
      "|    Mason|        1|         0| free|          1|1539318918000|\n",
      "+---------+---------+----------+-----+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#make sure DF looks like i expect\n",
    "df.select(['firstName', 'cancelled', 'downgraded', 'level', 'churn_phase', 'ts']).\\\n",
    "    orderBy(asc('ts')).where(df.firstName == 'Mason').show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 225\n"
     ]
    }
   ],
   "source": [
    "print(df.filter(col('downgrade_phase')==1).select('userId').dropDuplicates().count(), \n",
    "      df.filter(col('downgrade_phase')==0).select('userId').dropDuplicates().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 173\n"
     ]
    }
   ],
   "source": [
    "print(df.filter(col('churn_phase')==1).select('userId').dropDuplicates().count(),\n",
    "      df.filter(col('churn_phase')==0).select('userId').dropDuplicates().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(df.filter((col('churn_phase')==1) & (col('downgrade_phase')==1)).select('userId').dropDuplicates().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "print(df.filter((col('churn_phase')==1) & (col('downgrade_phase')==0) & (col('level')=='free'))\\\n",
    "      .select('userId').dropDuplicates().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-------------+-----------+---------------+\n",
      "|userId|     page|           ts|churn_phase|downgrade_phase|\n",
      "+------+---------+-------------+-----------+---------------+\n",
      "|    54| NextSong|1538353930000|          1|              1|\n",
      "|    54| NextSong|1538354180000|          1|              1|\n",
      "|    54| NextSong|1538354396000|          1|              1|\n",
      "|    54| NextSong|1538354739000|          1|              1|\n",
      "|    54|Downgrade|1538354749000|          1|              1|\n",
      "|    54| NextSong|1538354985000|          1|              1|\n",
      "|    54| NextSong|1538355254000|          1|              1|\n",
      "|    54|Thumbs Up|1538355255000|          1|              1|\n",
      "|100009| NextSong|1538374371000|          1|              1|\n",
      "|100009| NextSong|1538374563000|          1|              1|\n",
      "+------+---------+-------------+-----------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((col('churn_phase')==1) & (col('downgrade_phase')==1))\\\n",
    ".select('userId', 'page', 'ts', 'churn_phase', 'downgrade_phase').orderBy(col('ts')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+\n",
      "|summary|           userId|             count|\n",
      "+-------+-----------------+------------------+\n",
      "|  count|                9|                 9|\n",
      "|   mean|88917.88888888889| 641.6666666666666|\n",
      "| stddev|78156.17104945144|440.78254275776396|\n",
      "|    min|           100009|               308|\n",
      "|    max|               54|              1662|\n",
      "+-------+-----------------+------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.filter((col('churn_phase')==1) & (col('downgrade_phase')==1))\\\n",
    "      .groupBy('userId').count().describe().show()) #number of actions of users who downgraded and churned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "print(df.filter((col('churn_phase')==1) & (col('downgrade_phase')==0) & (col('level')=='paid'))\\\n",
    "      .select('userId').dropDuplicates().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|      avg(length)|\n",
      "+-----------------+\n",
      "|248.6327956440622|\n",
      "+-----------------+\n",
      "\n",
      "+------------------+\n",
      "|       avg(length)|\n",
      "+------------------+\n",
      "|249.20913538880808|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col('churn_phase')==1).agg({'length':'mean'}).show()\n",
    "df.filter(col('churn_phase')==0).agg({'length':'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|     F|19168|\n",
      "|     M|25696|\n",
      "+------+-----+\n",
      "\n",
      "+------+------+\n",
      "|gender| count|\n",
      "+------+------+\n",
      "|     F|135410|\n",
      "|     M| 97880|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col('churn_phase')==1).groupBy('gender').count().show()\n",
    "df.filter(col('churn_phase')==0).groupBy('gender').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "song=udf(lambda x : int(x=='NextSong'), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36394 191714\n"
     ]
    }
   ],
   "source": [
    "print(df.filter((col('churn_phase')==1) & (df.page == 'NextSong')).count(),\n",
    "df.filter((col('churn_phase')==0) & (df.page == 'NextSong')).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|   avg(songPlayed)|\n",
      "+------------------+\n",
      "|0.8112072039942939|\n",
      "+------------------+\n",
      "\n",
      "+------------------+\n",
      "|   avg(songPlayed)|\n",
      "+------------------+\n",
      "|0.8217840456084702|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col('churn_phase')==1).withColumn('songPlayed', song(col('page'))).agg({'songPlayed':'mean'}).show()\n",
    "df.filter(col('churn_phase')==0).withColumn('songPlayed', song(col('page'))).agg({'songPlayed':'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|gender|   avg(songPlayed)|\n",
      "+------+------------------+\n",
      "|     F|0.8055091819699499|\n",
      "|     M|0.8154576587795765|\n",
      "+------+------------------+\n",
      "\n",
      "+------+------------------+\n",
      "|gender|   avg(songPlayed)|\n",
      "+------+------------------+\n",
      "|     F|0.8216232183738277|\n",
      "|     M|0.8220065386187168|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col('churn_phase')==1).withColumn('songPlayed', song(col('page'))).groupBy('gender').agg({'songPlayed':'mean'}).show()\n",
    "df.filter(col('churn_phase')==0).withColumn('songPlayed', song(col('page'))).groupBy('gender').agg({'songPlayed':'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+\n",
      "|level_gender|    F|    M|\n",
      "+------------+-----+-----+\n",
      "|        paid|13925|18551|\n",
      "|        free| 5243| 7145|\n",
      "+------------+-----+-----+\n",
      "\n",
      "+------------+------+-----+\n",
      "|level_gender|     F|    M|\n",
      "+------------+------+-----+\n",
      "|        paid|110981|78976|\n",
      "|        free| 24429|18904|\n",
      "+------------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col('churn_phase')==1).crosstab('level', 'gender').show()\n",
    "df.filter(col('churn_phase')==0).crosstab('level', 'gender').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|avg(itemInSession)|\n",
      "+------------------+\n",
      "|109.23299304564907|\n",
      "+------------------+\n",
      "\n",
      "+------------------+\n",
      "|avg(itemInSession)|\n",
      "+------------------+\n",
      "| 115.9888465000643|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col('churn_phase')==1).agg({'itemInSession':'mean'}).show()\n",
    "df.filter(col('churn_phase')==0).agg({'itemInSession':'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_visit=udf(lambda x : int(x=='Home'), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|avg(count(songPeriod))|\n",
      "+----------------------+\n",
      "|      22.6612702366127|\n",
      "+----------------------+\n",
      "\n",
      "+----------------------+\n",
      "|avg(count(songPeriod))|\n",
      "+----------------------+\n",
      "|     23.79175974187143|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cusum = df.filter((df.page == 'NextSong') | (df.page == 'Home')) \\\n",
    "    .select('userID', 'page', 'ts', 'churn_phase') \\\n",
    "    .withColumn('homevisit', home_visit(col('page'))) \\\n",
    "    .withColumn('songPeriod', Fsum('homevisit').over(windowval))\n",
    "\n",
    "cusum.filter((cusum.churn_phase == 1) &(cusum.page == 'NextSong')) \\\n",
    "    .groupBy('userID', 'songPeriod') \\\n",
    "    .agg({'songPeriod':'count'}) \\\n",
    "    .agg({'count(songPeriod)':'avg'}).show()\n",
    "\n",
    "cusum.filter((cusum.churn_phase == 0) &(cusum.page == 'NextSong')) \\\n",
    "    .groupBy('userID', 'songPeriod') \\\n",
    "    .agg({'songPeriod':'count'}) \\\n",
    "    .agg({'count(songPeriod)':'avg'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = lambda i: i * 86400 \n",
    "daywindow = Window.partitionBy('userId', 'date').orderBy(desc('ts')).rangeBetween(Window.unboundedPreceding, 0)\n",
    "get_day = udf(lambda x: datetime.datetime.fromtimestamp(x/1000), DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('date', get_day(col('ts')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+\n",
      "|summary|           userId|            count|\n",
      "+-------+-----------------+-----------------+\n",
      "|  count|              499|              499|\n",
      "|   mean|77394.81563126252|72.93386773547094|\n",
      "| stddev|90869.89716037885|71.24764235703725|\n",
      "|    min|           100001|                1|\n",
      "|    max|               87|              346|\n",
      "+-------+-----------------+-----------------+\n",
      "\n",
      "+-------+------------------+-----------------+\n",
      "|summary|            userId|            count|\n",
      "+-------+------------------+-----------------+\n",
      "|  count|              2559|             2559|\n",
      "|   mean| 64501.19812426729|74.91754591637358|\n",
      "| stddev|114144.74005493976|74.02407951472078|\n",
      "|    min|                10|                1|\n",
      "|    max|                99|              360|\n",
      "+-------+------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"song = udf(lambda x: int(x=='NextSong'), IntegerType())\\ndf.select('userId', 'page', 'ts')    .withColumn('playSong', song('page'))    .withColumn('date', get_day(col('ts')))    .groupBy('userId', 'date').agg({'playSong':'sum'}).orderBy('userId').show()\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter((df.page=='NextSong')&(df.churn_phase==1)).select('userId', 'page', 'ts')\\\n",
    "    .withColumn('date', get_day(col('ts'))).groupBy('userId', 'date').count().describe().show()\n",
    "\n",
    "df.filter((df.page=='NextSong')&(df.churn_phase==0)).select('userId', 'page', 'ts')\\\n",
    "    .withColumn('date', get_day(col('ts'))).groupBy('userId', 'date').count().describe().show()\n",
    "\n",
    "'''song = udf(lambda x: int(x=='NextSong'), IntegerType())\n",
    "df.select('userId', 'page', 'ts')\\\n",
    "    .withColumn('playSong', song('page'))\\\n",
    "    .withColumn('date', get_day(col('ts')))\\\n",
    "    .groupBy('userId', 'date').agg({'playSong':'sum'}).orderBy('userId').show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------------------+\n",
      "|userId|      date|       avg(length)|\n",
      "+------+----------+------------------+\n",
      "|    10|2018-10-08|253.83504999999994|\n",
      "|    10|2018-10-29|270.73509571428576|\n",
      "|    10|2018-11-03|245.26612222222226|\n",
      "|    10|2018-11-16|293.21532833333333|\n",
      "|    10|2018-10-18| 247.2143123569023|\n",
      "|    10|2018-10-19|233.71678607142854|\n",
      "|    10|2018-11-15| 263.1104196721311|\n",
      "|    10|2018-11-19|242.81766166666662|\n",
      "|   100|2018-11-09|         242.06322|\n",
      "|   100|2018-11-16|214.68253833333335|\n",
      "|   100|2018-10-02|249.33048538461537|\n",
      "|   100|2018-11-30| 248.2140891358026|\n",
      "|   100|2018-10-09|  255.398004351852|\n",
      "|   100|2018-10-24|249.15869876190476|\n",
      "|   100|2018-11-05| 260.4105830000001|\n",
      "|   100|2018-10-04|244.25851054945056|\n",
      "|   100|2018-10-23|252.06774932203396|\n",
      "|   100|2018-10-25| 242.9726947368421|\n",
      "|   100|2018-10-07|209.02441733333336|\n",
      "|   100|2018-11-21|245.85800820512821|\n",
      "+------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "song = udf(lambda x: int(x=='NextSong'), IntegerType())\n",
    "df.filter(df.page=='NextSong').select('userId', 'page', 'ts', 'length')\\\n",
    "    .withColumn('playSong', song('page'))\\\n",
    "    .withColumn('date', get_day(col('ts')))\\\n",
    "    .groupBy('userId', 'date').agg({'length':'avg'}).orderBy('userId').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+\n",
      "|summary|           userId|      avg(length)|\n",
      "+-------+-----------------+-----------------+\n",
      "|  count|              499|              499|\n",
      "|   mean|77394.81563126252|248.0824544167563|\n",
      "| stddev|90869.89716037885|23.07628700860933|\n",
      "|    min|           100001|         45.63546|\n",
      "|    max|               87|388.1704313333333|\n",
      "+-------+-----------------+-----------------+\n",
      "\n",
      "+-------+------------------+------------------+\n",
      "|summary|            userId|       avg(length)|\n",
      "+-------+------------------+------------------+\n",
      "|  count|              2559|              2559|\n",
      "|   mean| 64501.19812426729|249.48107841849443|\n",
      "| stddev|114144.74005493976|27.895620078076377|\n",
      "|    min|                10|         139.51955|\n",
      "|    max|                99|         998.42567|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.page=='NextSong')&(df.churn_phase==1)).select('userId', 'page', 'ts', 'length')\\\n",
    "    .withColumn('date', get_day(col('ts')))\\\n",
    "    .groupBy('userId', 'date').agg({'length':'avg'}).describe().show()\n",
    "\n",
    "df.filter((df.page=='NextSong')&(df.churn_phase==0)).select('userId', 'page', 'ts', 'length')\\\n",
    "    .withColumn('date', get_day(col('ts')))\\\n",
    "    .groupBy('userId', 'date').agg({'length':'avg'}).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|            userId|       count(page)|\n",
      "+-------+------------------+------------------+\n",
      "|  count|                30|                30|\n",
      "|   mean| 90038.96666666666|1.0666666666666667|\n",
      "| stddev|102864.62817988047|0.2537081317024624|\n",
      "|    min|            100001|                 1|\n",
      "|    max|                73|                 2|\n",
      "+-------+------------------+------------------+\n",
      "\n",
      "+-------+------------------+-------------------+\n",
      "|summary|            userId|        count(page)|\n",
      "+-------+------------------+-------------------+\n",
      "|  count|               200|                200|\n",
      "|   mean|         56066.625|                1.1|\n",
      "| stddev|110994.57366660556|0.31702131247412063|\n",
      "|    min|               100|                  1|\n",
      "|    max|                98|                  3|\n",
      "+-------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.page=='Error')&(df.churn_phase==1)).select('userId', 'page', 'ts', 'length')\\\n",
    "    .withColumn('date', get_day(col('ts')))\\\n",
    "    .groupBy('userId', 'date').agg({'page':'count'}).describe().show()\n",
    "\n",
    "df.filter((df.page=='Error')&(df.churn_phase==0)).select('userId', 'page', 'ts', 'length')\\\n",
    "    .withColumn('date', get_day(col('ts')))\\\n",
    "    .groupBy('userId', 'date').agg({'page':'count'}).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: string, avg(count(page)): double]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter((df.page=='Help')&(df.churn_phase==1)).select('userId', 'page', 'ts', 'length')\\\n",
    "    .withColumn('date', get_day(col('ts')))\\\n",
    "    .groupBy('userId', 'date').agg({'page':'count'}).groupBy('userId').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+\n",
      "|summary|           userId|       count(page)|\n",
      "+-------+-----------------+------------------+\n",
      "|  count|              171|               171|\n",
      "|   mean|62613.21052631579|1.3976608187134503|\n",
      "| stddev| 88778.9002607509|0.8078987426633197|\n",
      "|    min|           100001|                 1|\n",
      "|    max|               87|                 6|\n",
      "+-------+-----------------+------------------+\n",
      "\n",
      "+-------+------------------+------------------+\n",
      "|summary|            userId|       count(page)|\n",
      "+-------+------------------+------------------+\n",
      "|  count|               816|               816|\n",
      "|   mean| 63176.38602941176|1.4889705882352942|\n",
      "| stddev|115512.27837413888|0.8636495080491383|\n",
      "|    min|                10|                 1|\n",
      "|    max|                99|                 8|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.page=='Help')&(df.churn_phase==1)).select('userId', 'page', 'ts', 'length')\\\n",
    "    .withColumn('date', get_day(col('ts')))\\\n",
    "    .groupBy('userId', 'date').agg({'page':'count'}).describe().show()\n",
    "\n",
    "df.filter((df.page=='Help')&(df.churn_phase==0)).select('userId', 'page', 'ts', 'length')\\\n",
    "    .withColumn('date', get_day(col('ts')))\\\n",
    "    .groupBy('userId', 'date').agg({'page':'count'}).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other options for future analysis:\n",
    "- how many users went to `downgrade` but did not `submit downgrade`\n",
    "- how many users went to `cancel` but did not `submit cancel`\n",
    "- rates of `add friend`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "|100010|\n",
      "|200002|\n",
      "+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dataframe of user ids to merge onto\n",
    "data = spark.read.json(\"mini_sparkify_event_data.json\")\n",
    "data = df.where((df.userId != \"\") | (df.sessionId != \"\")).select('userId').dropDuplicates()\n",
    "data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data\n",
    "df = df.where((df.userId != \"\") | (df.sessionId != \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = udf(lambda x: int(x==\"Cancellation Confirmation\"), IntegerType())\n",
    "downgrade_churn = udf(lambda x: int(x==\"Submit Downgrade\"), IntegerType())\n",
    "visited_downgrade = udf(lambda x: int(x=='Downgrade'), IntegerType())\n",
    "visited_cancel = udf(lambda x: int(x=='Cancel'), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgThumbsUp = df.filter(df.page=='Thumbs Up').select('userId', 'page', 'ts')\\\n",
    "    .withColumn('date', get_day(col('ts')))\\\n",
    "    .groupBy('userId', 'date').agg({'page':'count'}).groupBy('userId')\\\n",
    "    .mean().withColumnRenamed('avg(count(page))', 'avgThumbsUp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgThumbsDown = df.filter(df.page=='Thumbs Down').select('userId', 'page', 'ts')\\\n",
    "    .withColumn('date', get_day(col('ts')))\\\n",
    "    .groupBy('userId', 'date').agg({'page':'count'}).groupBy('userId')\\\n",
    "    .mean().withColumnRenamed('avg(count(page))', 'avgThumbsDown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFriends = df.filter(df.page=='Add Friend').select('userId', 'page')\\\n",
    "    .groupBy('userId').count().withColumnRenamed('count', 'numFriends')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Window.partitionBy(\"userId\", \"sessionId\").orderBy(desc(\"ts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "song = udf(lambda x: int(x=='NextSong'), IntegerType())\n",
    "days = lambda i: i * 86400 \n",
    "daywindow = Window.partitionBy('userId', 'date').orderBy(desc('ts')).rangeBetween(Window.unboundedPreceding, 0)\n",
    "get_day = udf(lambda x: datetime.datetime.fromtimestamp(x/1000), DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipped = udf(lambda x: int(x!=0), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------------+---------+---------+-------------+----+-----------+------+-----------+-------------+-------+\n",
      "|userId|      page|           ts|   length|sessionId|itemInSession|song|nextActSong|tsDiff|timeSkipped|roundedLength|skipped|\n",
      "+------+----------+-------------+---------+---------+-------------+----+-----------+------+-----------+-------------+-------+\n",
      "|    10|  NextSong|1538965220000|265.53424|        9|            0|   1|          1| 265.0|        0.0|          265|      0|\n",
      "|    10|  NextSong|1538965485000| 200.4371|        9|            1|   1|          1| 200.0|        0.0|          200|      0|\n",
      "|    10|  NextSong|1538965685000|208.03873|        9|            2|   1|          1| 208.0|        0.0|          208|      0|\n",
      "|    10|  NextSong|1538965893000|526.44526|        9|            3|   1|          1| 526.0|        0.0|          526|      0|\n",
      "|    10|  NextSong|1538966419000|272.06485|        9|            4|   1|          1| 272.0|        0.0|          272|      0|\n",
      "|    10|  NextSong|1538966691000|207.01995|        9|            5|   1|          1| 207.0|        0.0|          207|      0|\n",
      "|    10|  NextSong|1538966898000|106.31791|        9|            6|   1|          1| 106.0|        0.0|          106|      0|\n",
      "|    10|  NextSong|1538967004000| 239.3073|        9|            8|   1|          1| 239.0|        0.0|          239|      0|\n",
      "|    10|  NextSong|1538967243000|264.85506|        9|            9|   1|          1| 264.0|        0.0|          264|      0|\n",
      "|    10|  NextSong|1538967507000|363.41506|        9|           10|   1|          0|   1.0|      362.0|          363|      1|\n",
      "|    10|Add Friend|1538967508000|     null|        9|           11|   0|          0|   1.0|       null|         null|      1|\n",
      "|    10|Add Friend|1538967509000|     null|        9|           12|   0|          1| 361.0|       null|         null|      1|\n",
      "|    10|  NextSong|1538967870000|240.01261|        9|           13|   1|          1| 240.0|        0.0|          240|      0|\n",
      "|    10|  NextSong|1538968110000|206.96771|        9|           14|   1|          1| 206.0|        0.0|          206|      0|\n",
      "|    10|  NextSong|1538968316000|432.84853|        9|           16|   1|          1| 432.0|        0.0|          432|      0|\n",
      "|    10|  NextSong|1538968748000|159.05914|        9|           17|   1|          1| 159.0|        0.0|          159|      0|\n",
      "|    10|  NextSong|1538968907000|233.35138|        9|           18|   1|          1| 233.0|        0.0|          233|      0|\n",
      "|    10|  NextSong|1538969140000|222.53669|        9|           19|   1|          1| 222.0|        0.0|          222|      0|\n",
      "|    10|  NextSong|1538969362000|172.69506|        9|           20|   1|          1| 172.0|        0.0|          172|      0|\n",
      "|    10|  NextSong|1538969534000|348.57751|        9|           21|   1|          1| 348.0|        0.0|          348|      0|\n",
      "|    10|  NextSong|1538969882000|247.06567|        9|           22|   1|          1| 247.0|        0.0|          247|      0|\n",
      "|    10|  NextSong|1538970129000|299.44118|        9|           23|   1|          1| 299.0|        0.0|          299|      0|\n",
      "|    10|  NextSong|1538970428000|177.21424|        9|           24|   1|          1| 177.0|        0.0|          177|      0|\n",
      "|    10|  NextSong|1538970605000|263.60118|        9|           25|   1|          1| 263.0|        0.0|          263|      0|\n",
      "|    10|  NextSong|1538970868000|190.24934|        9|           26|   1|          0|   1.0|      189.0|          190|      1|\n",
      "|    10|    Logout|1538970869000|     null|        9|           27|   0|          0|  31.0|       null|         null|      1|\n",
      "|    10|      Home|1538970900000|     null|        9|           30|   0|          1| 158.0|       null|         null|      1|\n",
      "|    10|  NextSong|1538971058000|383.73832|        9|           31|   1|          1| 383.0|        0.0|          383|      0|\n",
      "|    10|  NextSong|1538971441000|484.93669|        9|           32|   1|          1| 484.0|        0.0|          484|      0|\n",
      "|    10|  NextSong|1538971925000|169.06404|        9|           33|   1|          1| 169.0|        0.0|          169|      0|\n",
      "|    10|  NextSong|1538972094000|114.12853|        9|           34|   1|          1| 114.0|        0.0|          114|      0|\n",
      "|    10|  NextSong|1538972208000|222.24934|        9|           35|   1|          1| 222.0|        0.0|          222|      0|\n",
      "|    10|  NextSong|1538972430000|280.60689|        9|           36|   1|          1| 280.0|        0.0|          280|      0|\n",
      "|    10|  NextSong|1538972710000|184.65914|        9|           37|   1|          1| 184.0|        0.0|          184|      0|\n",
      "|    10|  NextSong|1538972894000|215.53587|        9|           38|   1|          1| 215.0|        0.0|          215|      0|\n",
      "|    10|  NextSong|1538973109000|322.16771|        9|           39|   1|          1| 322.0|        0.0|          322|      0|\n",
      "|    10|  NextSong|1538973431000|205.45261|        9|           40|   1|          0|   9.0|      196.0|          205|      1|\n",
      "|    10|      Home|1538973440000|     null|        9|           41|   0|          1| 196.0|       null|         null|      1|\n",
      "|    10|  NextSong|1538973636000|334.23628|        9|           42|   1|          1| 334.0|        0.0|          334|      0|\n",
      "|    10|  NextSong|1538973970000|266.47465|        9|           43|   1|          1| 266.0|        0.0|          266|      0|\n",
      "|    10|  NextSong|1538974236000|246.62159|        9|           44|   1|          1| 246.0|        0.0|          246|      0|\n",
      "|    10|  NextSong|1538974482000|126.74567|        9|           45|   1|          1| 126.0|        0.0|          126|      0|\n",
      "|    10|  NextSong|1538974608000|295.47057|        9|           46|   1|          1| 295.0|        0.0|          295|      0|\n",
      "|    10|  NextSong|1538974903000|268.25098|        9|           47|   1|          1| 268.0|        0.0|          268|      0|\n",
      "|    10|  NextSong|1538975171000|232.22812|        9|           48|   1|          1| 232.0|        0.0|          232|      0|\n",
      "|    10|  NextSong|1538975403000|313.62567|        9|           49|   1|          1| 313.0|        0.0|          313|      0|\n",
      "|    10|  NextSong|1538975716000|307.46077|        9|           50|   1|          1| 307.0|        0.0|          307|      0|\n",
      "|    10|  NextSong|1538976023000|267.49342|        9|           51|   1|          0|   9.0|      258.0|          267|      1|\n",
      "|    10|      Home|1538976032000|     null|        9|           52|   0|          0|   6.0|       null|         null|      1|\n",
      "|    10|  Settings|1538976038000|     null|        9|           53|   0|          1| 252.0|       null|         null|      1|\n",
      "+------+----------+-------------+---------+---------+-------------+----+-----------+------+-----------+-------------+-------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('userId', 'page', 'ts', 'length', 'sessionId', 'itemInSession')\\\n",
    "    .where((df.page != 'Thumbs Up') & (df.page != 'Thumbs Down'))\\\n",
    "    .withColumn('song', song('page')).orderBy('userId', 'sessionId', 'itemInSession')\\\n",
    "    .withColumn('nextActSong', lag(col('song')).over(session))\\\n",
    "    .withColumn('tsDiff', (lag('ts').over(session)-col('ts'))/1000)\\\n",
    "    .withColumn('timeSkipped', (floor('length')-col('tsDiff')))\\\n",
    "    .withColumn('roundedLength', floor('length'))\\\n",
    "    .withColumn('skipped', skipped('timeSkipped'))\\\n",
    "    .orderBy('userId', 'sessionId', 'itemInSession')\\\n",
    "    .show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "process\n",
    "\n",
    "1. dont include thumbs up and down pages because that usually occurs while playing and does not change song\n",
    "2. create variable for if action is song\n",
    "3. check if next action is song - this will check to see if someone is skipping song or just leaving page\n",
    "4. get the difference in timestamp for next action song playing\n",
    "5. subtract the difference in timestamp from song length to see how much of song was skipped\n",
    "6. get descriptive stats\n",
    "'''\n",
    "\n",
    "skipping = df.select('userId', 'page', 'ts', 'length', 'sessionId', 'itemInSession')\\\n",
    "    .where((df.page != 'Thumbs Up') & (df.page != 'Thumbs Down'))\\\n",
    "    .withColumn('song', song('page')).orderBy('userId', 'sessionId', 'itemInSession')\\\n",
    "    .withColumn('nextActSong', lag(col('song')).over(session))\\\n",
    "    .withColumn('tsDiff', (lag('ts').over(session)-col('ts'))/1000)\\\n",
    "    .withColumn('timeSkipped', (floor('length')-col('tsDiff')))\\\n",
    "    .withColumn('roundedLength', floor('length'))\\\n",
    "    .where((col('song')==1) & ((col('nextActSong')!=0)&(col('timeSkipped')>=0)))\\\n",
    "    .withColumn('skipped', skipped('timeSkipped'))\\\n",
    "    .select('userId', 'timeSkipped', 'skipped', 'length', 'ts', 'tsDiff')\\\n",
    "    .groupBy('userId').agg({'skipped':'avg', 'timeSkipped':'avg'})\\\n",
    "    .withColumnRenamed('avg(skipped)', 'skipRate')\\\n",
    "    .withColumnRenamed('avg(timeSkipped)', 'avgTimeSkipped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avg daily visits to help site\n",
    "dailyHelpVisit = df.filter(df.page=='Help').select('userId', 'page', 'ts', 'length')\\\n",
    "    .withColumn('date', get_day(col('ts')))\\\n",
    "    .groupBy('userId', 'date').agg({'page':'count'}).groupBy('userId').mean()\\\n",
    "                     .withColumnRenamed('avg(count(page))', 'dailyHelpVisits')\n",
    "\n",
    "dailyErrors = df.filter(df.page=='Error').select('userId', 'page', 'ts', 'length')\\\n",
    "    .withColumn('date', get_day(col('ts')))\\\n",
    "    .groupBy('userId', 'date').agg({'page':'count'}).groupBy('userId').mean()\\\n",
    "                     .withColumnRenamed('avg(count(page))', 'dailyErrors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whether a user has downgraded\n",
    "churn = df.withColumn(\"downgraded\", downgrade_churn(\"page\"))\\\n",
    "    .withColumn(\"cancelled\", churn(\"page\"))\\\n",
    "    .withColumn('visited_cancel', visited_cancel('page'))\\\n",
    "    .withColumn('visited_downgrade', visited_downgrade('page'))\\\n",
    "    .select(['userId', 'downgraded', 'cancelled', 'visited_cancel', 'visited_downgrade'])\\\n",
    "    .groupBy('userId').sum()\\\n",
    "    .withColumnRenamed('sum(downgraded)', 'downgraded')\\\n",
    "    .withColumnRenamed('sum(cancelled)', 'cancelled')\\\n",
    "    .withColumnRenamed('sum(visited_cancel)', 'visited_cancel')\\\n",
    "    .withColumnRenamed('sum(visited_downgrade)', 'visited_downgrade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "free = udf(lambda x: int(x=='free'), IntegerType())\n",
    "paid = udf(lambda x: int(x=='paid'), IntegerType())\n",
    "\n",
    "user_level = df.select('userId', 'level').where((df.level=='free')|(df.level=='paid'))\\\n",
    "    .dropDuplicates()\\\n",
    "    .withColumn('free', free('level'))\\\n",
    "    .withColumn('paid', paid('level')).drop('level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_visit=udf(lambda x : int(x=='Home'), IntegerType())\n",
    "windowval = Window.partitionBy(\"userId\").orderBy(desc(\"ts\")).rangeBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "cusum = df.filter((df.page == 'NextSong') | (df.page == 'Home')) \\\n",
    "    .select('userID', 'page', 'ts') \\\n",
    "    .withColumn('homevisit', home_visit(col('page'))) \\\n",
    "    .withColumn('songPeriod', Fsum('homevisit').over(windowval))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgSongsTillHome = cusum.filter((cusum.page=='NextSong'))\\\n",
    "    .groupBy('userId', 'songPeriod')\\\n",
    "    .agg({'songPeriod':'count'}).drop('songPeriod').groupby('userId').mean()\\\n",
    "    .withColumnRenamed('avg(count(songPeriod))', 'avgSongsTillHome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.join(churn, on='userId')\\\n",
    "    .join(dailyHelpVisit, on='userId')\\\n",
    "    .join(dailyErrors, on='userId')\\\n",
    "    .join(user_level, on='userId')\\\n",
    "    .join(avgThumbsUp, on='userId')\\\n",
    "    .join(avgThumbsDown, on='userId')\\\n",
    "    .join(numFriends, on='userId')\\\n",
    "    .join(avgSongsTillHome, on='userId')\\\n",
    "    .join(skipping, on='userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- downgraded: long (nullable = true)\n",
      " |-- cancelled: long (nullable = true)\n",
      " |-- visited_cancel: long (nullable = true)\n",
      " |-- visited_downgrade: long (nullable = true)\n",
      " |-- dailyHelpVisits: double (nullable = true)\n",
      " |-- dailyErrors: double (nullable = true)\n",
      " |-- free: integer (nullable = true)\n",
      " |-- paid: integer (nullable = true)\n",
      " |-- avgThumbsUp: double (nullable = true)\n",
      " |-- avgThumbsDOwn: double (nullable = true)\n",
      " |-- numFriends: long (nullable = false)\n",
      " |-- avgSongsTillHome: double (nullable = true)\n",
      " |-- avgTimeSkipped: double (nullable = true)\n",
      " |-- skipRate: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, rest = df.randomSplit([0.85, 0.15], seed=42)\n",
    "validation, test = rest.randomSplit([0.5,0.5], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline=Pipeline(stages=[])\n",
    "paramGrid=ParamGridBuilder()\\\n",
    "    .addGrid()\\\n",
    "    .build()\n",
    "\n",
    "\n",
    "cross_val = CrossValidator(estimator=pipeline,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator= ,\n",
    "                           numFolds=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
