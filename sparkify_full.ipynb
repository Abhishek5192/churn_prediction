{"cells": [{"metadata": {"collapsed": true, "trusted": true}, "cell_type": "code", "source": "# Starter code\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, lit, udf, isnan, count, when, desc, sort_array, asc, avg, lag, floor\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import IntegerType, DateType\nfrom pyspark.sql.functions import sum as Fsum\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import MinMaxScaler #used because won't distort binary vars\nfrom pyspark.sql.types import DoubleType\nimport datetime\n\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.mllib.evaluation import MulticlassMetrics\nimport numpy as np\n\n# Create spark session\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Sparkify\") \\\n    .getOrCreate()\n\n# Read in full sparkify dataset\nevent_data = \"s3n://dsnd-sparkify/sparkify_event_data.json\"\ndata = spark.read.json(event_data)\ndata.head()", "execution_count": 111, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "177c6fcba01744068ee17b499b9b3ae3"}}, "metadata": {}}, {"output_type": "stream", "text": "An error was encountered:\nInvalid status code '404' from https://172.31.46.197:18888/sessions/0 with error payload: \"Session '0' not found.\"\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#define necessary functions\ndef feature_engineering(df):\n    '''\n    Create necessary features to use machine learning algorithms.\n    First loads data set from file\n    \n    Resulting DF Strucutre:\n    \n    root\n     |-- userId: string\n     |-- downgraded: long\n     |-- cancelled: long\n     |-- visited_cancel: long\n     |-- visited_downgrade: long\n     |-- dailyHelpVisits: double\n     |-- dailyErrors: double\n     |-- free: integer\n     |-- paid: integer\n     |-- avgThumbsUp: double\n     |-- avgThumbsDOwn: double\n     |-- numFriends: long\n     |-- avgSongsTillHome: double\n     |-- avgTimeSkipped: double\n     |-- skipRate: double\n    \n    Inputs\n        filepath (str) - path to json dataset on file\n        \n    Outputs\n        data - engineered dataset\n    '''\n    #dataframe of user ids to merge onto\n    users = df.where((df.userId != \"\") | (df.sessionId != \"\"))\\\n        .select('userId').dropDuplicates()\n    \n    #define custom functions\n    churn = udf(lambda x: int(x==\"Cancellation Confirmation\"), IntegerType())\n    downgrade_churn = udf(lambda x: int(x==\"Submit Downgrade\"), IntegerType())\n    visited_downgrade = udf(lambda x: int(x=='Downgrade'), IntegerType())\n    visited_cancel = udf(lambda x: int(x=='Cancel'), IntegerType())\n    song = udf(lambda x: int(x=='NextSong'), IntegerType())\n    days = lambda i: i * 86400 \n    get_day = udf(lambda x: datetime.datetime.fromtimestamp(x/1000), DateType())\n    skipped = udf(lambda x: int(x!=0), IntegerType())\n    free = udf(lambda x: int(x=='free'), IntegerType())\n    paid = udf(lambda x: int(x=='paid'), IntegerType())\n    home_visit=udf(lambda x : int(x=='Home'), IntegerType())\n    \n    #define windows\n    windowval = Window.partitionBy(\"userId\").orderBy(desc(\"ts\")).rangeBetween(Window.unboundedPreceding, 0)\n    session = Window.partitionBy(\"userId\", \"sessionId\").orderBy(desc(\"ts\"))\n    daywindow = Window.partitionBy('userId', 'date').orderBy(desc('ts'))\\\n        .rangeBetween(Window.unboundedPreceding, 0)\n\n    avgThumbsUp = df.filter(df.page=='Thumbs Up')\\\n        .select('userId', 'page', 'ts')\\\n        .withColumn('date', get_day(col('ts')))\\\n        .groupBy('userId', 'date').agg({'page':'count'}).groupBy('userId')\\\n        .mean().withColumnRenamed('avg(count(page))', 'avgThumbsUp')\n    \n    avgThumbsDown = df.filter(df.page=='Thumbs Down')\\\n        .select('userId', 'page', 'ts')\\\n        .withColumn('date', get_day(col('ts')))\\\n        .groupBy('userId', 'date').agg({'page':'count'})\\\n        .groupBy('userId').mean()\\\n        .withColumnRenamed('avg(count(page))', 'avgThumbsDown')\n    \n    numFriends = df.filter(df.page=='Add Friend')\\\n        .select('userId', 'page')\\\n        .groupBy('userId').count().withColumnRenamed('count', 'numFriends')\n    \n    '''\n    process for calculating skipping variables\n\n    1. dont include thumbs up and down pages because that usually occurs \n        while playing and does not change song\n    2. create variable for if action is song\n    3. check if next action is song - this will check to see if someone is \n        skipping song or just leaving page\n    4. get the difference in timestamp for next action song playing\n    5. subtract the difference in timestamp from song length to see \n        how much of song was skipped\n    6. get descriptive stats\n    '''\n\n    skipping = df.select('userId', 'page', 'ts', 'length', 'sessionId', 'itemInSession')\\\n        .where((df.page != 'Thumbs Up') & (df.page != 'Thumbs Down'))\\\n        .withColumn('song', song('page')).orderBy('userId', 'sessionId', 'itemInSession')\\\n        .withColumn('nextActSong', lag(col('song')).over(session))\\\n        .withColumn('tsDiff', (lag('ts').over(session)-col('ts'))/1000)\\\n        .withColumn('timeSkipped', (floor('length')-col('tsDiff')))\\\n        .withColumn('roundedLength', floor('length'))\\\n        .where((col('song')==1) & ((col('nextActSong')!=0)&(col('timeSkipped')>=0)))\\\n        .withColumn('skipped', skipped('timeSkipped'))\\\n        .select('userId', 'timeSkipped', 'skipped', 'length', 'ts', 'tsDiff')\\\n        .groupBy('userId').agg({'skipped':'avg', 'timeSkipped':'avg'})\\\n        .withColumnRenamed('avg(skipped)', 'skipRate')\\\n        .withColumnRenamed('avg(timeSkipped)', 'avgTimeSkipped')\n    \n#avg daily visits to help site\n    dailyHelpVisit = df.filter(df.page=='Help')\\\n        .select('userId', 'page', 'ts', 'length')\\\n        .withColumn('date', get_day(col('ts')))\\\n        .groupBy('userId', 'date').agg({'page':'count'})\\\n        .groupBy('userId').mean()\\\n         .withColumnRenamed('avg(count(page))', 'dailyHelpVisits')\n\n    dailyErrors = df.filter(df.page=='Error')\\\n        .select('userId', 'page', 'ts', 'length')\\\n        .withColumn('date', get_day(col('ts')))\\\n        .groupBy('userId', 'date').agg({'page':'count'})\\\n        .groupBy('userId').mean()\\\n        .withColumnRenamed('avg(count(page))', 'dailyErrors')\n    \n    #whether a user has downgraded\n    churn = df.withColumn(\"downgraded\", downgrade_churn(\"page\"))\\\n        .withColumn(\"cancelled\", churn(\"page\"))\\\n        .withColumn('visited_cancel', visited_cancel('page'))\\\n        .withColumn('visited_downgrade', visited_downgrade('page'))\\\n        .select(['userId', 'downgraded', 'cancelled', 'visited_cancel', 'visited_downgrade'])\\\n        .groupBy('userId').sum()\\\n        .withColumnRenamed('sum(downgraded)', 'downgraded')\\\n        .withColumnRenamed('sum(cancelled)', 'cancelled')\\\n        .withColumnRenamed('sum(visited_cancel)', 'visited_cancel')\\\n        .withColumnRenamed('sum(visited_downgrade)', 'visited_downgrade')\n\n    user_level = df.select('userId', 'level')\\\n        .where((df.level=='free')|(df.level=='paid'))\\\n        .dropDuplicates()\\\n        .withColumn('free', free('level'))\\\n        .withColumn('paid', paid('level')).drop('level')\n\n    cusum = df.filter((df.page == 'NextSong') | (df.page == 'Home')) \\\n        .select('userID', 'page', 'ts') \\\n        .withColumn('homevisit', home_visit(col('page'))) \\\n        .withColumn('songPeriod', Fsum('homevisit').over(windowval))\\\n    \n    avgSongsTillHome = cusum.filter((cusum.page=='NextSong'))\\\n        .groupBy('userId', 'songPeriod')\\\n        .agg({'songPeriod':'count'}).drop('songPeriod')\\\n        .groupby('userId').mean()\\\n        .withColumnRenamed('avg(count(songPeriod))', 'avgSongsTillHome')\n    \n    df = users.join(churn, on='userId')\\\n        .join(dailyHelpVisit, on='userId')\\\n        .join(dailyErrors, on='userId')\\\n        .join(user_level, on='userId')\\\n        .join(avgThumbsUp, on='userId')\\\n        .join(avgThumbsDown, on='userId')\\\n        .join(numFriends, on='userId')\\\n        .join(avgSongsTillHome, on='userId')\\\n        .join(skipping, on='userId')\n    return df\n\ndef feature_scaling(df):\n    feature_cols = df.drop('userId', 'cancelled').columns\n    assembler = VectorAssembler(inputCols=feature_cols,\\\n                                outputCol='feature_vec')\n    \n    #pyspark.ml expects target column to be names: 'labelCol', w/ type: Double\n    df = df.withColumn(\"label\", df[\"cancelled\"].cast(DoubleType()))\n    \n    #pyspark default name for features vector column: 'featuresCol'\n    minmaxscaler = MinMaxScaler(inputCol=\"feature_vec\", outputCol=\"features\")\n    \n    df = assembler.transform(df)\n    minmaxscaler_model = minmaxscaler.fit(df)\n    scaled_df = minmaxscaler_model.transform(df)\n    return scaled_df\n\ndef custom_evaluation(pred, model_name):\n    '''\n    Perform custom evaluation of predictions\n    \n    1.inspect with PySpark.ML evaluator (will use for pipeline)\n    2. use RDD-API; PySpark.MLLib to get metrics based on predictions \n    3. display confusion matrix\n    \n    Inspiration from: https://chih-ling-hsu.github.io/2018/09/17/spark-mllib\n    https://spark.apache.org/docs/2.2.0/mllib-evaluation-metrics.html\n    https://stackoverflow.com/questions/35572000/how-can-i-plot-a-confusion-matrix\n    \n    Inputs\n        preds(PySpark.ml.DataFrame) - predictions from model\n    '''\n    #want to evaluate binary class, auc_pr is best for imbalanced classes\n    tn_sum = pred.filter((pred.label == 0)&(pred.prediction==0)).count() #true negative\n    fn_sum = pred.filter((pred.label == 1)&(pred.prediction==0)).count() #false negative\n    fp_sum = pred.filter((pred.label == 0)&(pred.prediction==1)).count() #false positive\n    tp_sum = pred.filter((pred.label == 1)&(pred.prediction==1)).count() #true positive\n\n    print(\"{} \\n | tn:{}| fn:{}| fp:{}| tp:{}\".format(model_name, tn_sum, fn_sum, fp_sum, tp_sum))", "execution_count": 89, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c5ddc219e40b4306a5675fa104c87b6e"}}, "metadata": {}}]}, {"metadata": {"collapsed": true, "trusted": true}, "cell_type": "code", "source": "#prepare data for ML\ndf = feature_engineering(data)\ndf_scaled = feature_scaling(df)\ndf.persist()\ndf_scaled.persist() #keep operations in memory to reduce computational time when testing", "execution_count": 12, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "467c2e2f4b6142109a6378edea9c01e6"}}, "metadata": {}}, {"output_type": "stream", "text": "----------------------------------------\nException happened during processing of request from ('127.0.0.1', 54034)\n----------------------------------------\nDataFrame[userId: string, downgraded: bigint, cancelled: bigint, visited_cancel: bigint, visited_downgrade: bigint, dailyHelpVisits: double, dailyErrors: double, free: int, paid: int, avgThumbsUp: double, avgThumbsDown: double, numFriends: bigint, avgSongsTillHome: double, avgTimeSkipped: double, skipRate: double, label: double, feature_vec: vector, features: vector]\nTraceback (most recent call last):\n  File \"/usr/lib64/python2.7/SocketServer.py\", line 290, in _handle_request_noblock\n    self.process_request(request, client_address)\n  File \"/usr/lib64/python2.7/SocketServer.py\", line 318, in process_request\n    self.finish_request(request, client_address)\n  File \"/usr/lib64/python2.7/SocketServer.py\", line 331, in finish_request\n    self.RequestHandlerClass(request, client_address, self)\n  File \"/usr/lib64/python2.7/SocketServer.py\", line 652, in __init__\n    self.handle()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 266, in handle\n    poll(authenticate_and_accum_updates)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 241, in poll\n    if func():\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 254, in authenticate_and_accum_updates\n    received_token = self.rfile.read(len(auth_token))\nTypeError: object of type 'NoneType' has no len()", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#split data for training\ntrain, rest = df_scaled.randomSplit([0.85, 0.15], seed=42)\nvalidation, test = rest.randomSplit([0.5,0.5], seed=42)", "execution_count": 18, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "a4de5a2a6b7440f98727f0e8de14f2e2"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#random forest classifier model\nfrom pyspark.ml.classification import RandomForestClassifier\nrf = RandomForestClassifier(numTrees=10)\nrf_model = rf.fit(train)\nrf_preds = rf_model.transform(validation)\ncustom_evaluation(rf_preds, 'Random Forest')", "execution_count": 93, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2b4346903d7a4b998de6acccd5ab3c66"}}, "metadata": {}}, {"output_type": "stream", "text": "Random Forest \n | tn:1043| fn:0| fp:0| tp:279", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#gradient boosted trees (ie ada boost)\nfrom pyspark.ml.classification import GBTClassifier\ngbtrees = GBTClassifier(maxIter=10)\ngbtree_model = gbtrees.fit(train)\ngbtree_preds = gbtree_model.transform(validation)\ncustom_evaluation(gbtree_preds, 'Gradient Boosted Trees')", "execution_count": 94, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f47b1a01a44f4904904b6834ad130ac6"}}, "metadata": {}}, {"output_type": "stream", "text": "Gradient Boosted Trees \n | tn:1043| fn:0| fp:0| tp:279", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "gbt_paramMap=gbtrees.extractParamMap()\ngbt_paramexpl = gbtrees.explainParams()", "execution_count": 104, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "af6ba0c26e314b91bd585622e4d5473f"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#SVM: https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-support-vector-machine\nfrom pyspark.ml.classification import LinearSVC\nsvm = LinearSVC(maxIter=10, regParam=0.1)\nsvm_model=svm.fit(train)\nsvm_preds=svm_model.transform(validation)\ncustom_evaluation(svm_preds, 'Support Vector Machine')", "execution_count": 95, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "93b9093305a34e4499a42b63583acd91"}}, "metadata": {}}, {"output_type": "stream", "text": "Support Vector Machine \n | tn:1043| fn:0| fp:0| tp:279", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#logistic regression model\nfrom pyspark.ml.classification import LogisticRegression\nlogReg = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\nlrModel = logReg.fit(train)\nlr_preds = lrModel.transform(validation)\n\ncustom_evaluation(lr_preds, 'Logistic Regression')", "execution_count": 92, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "ae5583108a534e13af487ba392bd2da2"}}, "metadata": {}}, {"output_type": "stream", "text": "Logistic Regression \n | tn:1043| fn:279| fp:0| tp:0", "name": "stdout"}]}, {"metadata": {"collapsed": true, "trusted": true}, "cell_type": "code", "source": "import time\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n#gbtree cross val\n#https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.GBTClassifier\ngbtrees_paramGrid = ParamGridBuilder() \\\n    .addGrid(gbtrees.featureSubsetStrategy,['all', 'onethird', 'sqrt', 'log2'])\\\n    .addGrid(gbtrees.maxDepth,[1,5,10])\\\n    .addGrid(gbtrees.maxIter,[5,10,20])\\\n    .addGrid(gbtrees.stepSize,[0.1, 0.25, 0.5])\\\n    .addGrid(gbtrees.minInstancesPerNode,[1,3,5])\\\n    .addGrid(gbtrees.maxBins,[20,32,64])\\\n    .build()\n\ngbtrees_crossval = CrossValidator(estimator=gbtrees,\n                          estimatorParamMaps=gbtrees_paramGrid,\n                          evaluator=BinaryClassificationEvaluator(),\n                          numFolds=4)\n\nstart=time.time()\ngbt_cv_model = gbtrees_crossval.fit(train)\nend=time.time()\nprint(\"time for cv: {}\".format(end-start))\n\ngbt_cv_pred = gbt_cv_model.transform(test)", "execution_count": 107, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "ad1f8fcc8b94492c8859ba70e818ccb6"}}, "metadata": {}}, {"output_type": "stream", "text": "An error was encountered:\nInvalid status code '400' from https://172.31.46.197:18888/sessions/0/statements/106 with error payload: \"requirement failed: Session isn't active.\"\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "pysparkkernel", "display_name": "PySpark", "language": ""}, "language_info": {"name": "pyspark", "mimetype": "text/x-python", "codemirror_mode": {"name": "python", "version": 2}, "pygments_lexer": "python2"}}, "nbformat": 4, "nbformat_minor": 2}