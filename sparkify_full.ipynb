{"cells": [{"metadata": {"trusted": true}, "cell_type": "code", "source": "# Starter code\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, lit, udf, isnan, count, when, desc, sort_array, asc, avg, lag, floor\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import IntegerType, DateType\nfrom pyspark.sql.functions import sum as Fsum\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import MinMaxScaler #used because won't distort binary vars\nfrom pyspark.sql.types import DoubleType\nimport datetime\n\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.mllib.evaluation import MulticlassMetrics\nimport numpy as np\n\n# Create spark session\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Sparkify\") \\\n    .getOrCreate()\n\n# Read in full sparkify dataset\nevent_data = \"s3n://dsnd-sparkify/sparkify_event_data.json\"\ndata = spark.read.json(event_data)\ndata.head()", "execution_count": 1, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "bc8b530fa43f4d12a523c9effc2315a4"}}, "metadata": {}}, {"output_type": "stream", "text": "Starting Spark application\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1548207771803_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-37-87.us-east-2.compute.internal:20888/proxy/application_1548207771803_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-37-125.us-east-2.compute.internal:8042/node/containerlogs/container_1548207771803_0001_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "text": "SparkSession available as 'spark'.\nRow(artist=u'Popol Vuh', auth=u'Logged In', firstName=u'Shlok', gender=u'M', itemInSession=278, lastName=u'Johnson', length=524.32934, level=u'paid', location=u'Dallas-Fort Worth-Arlington, TX', method=u'PUT', page=u'NextSong', registration=1533734541000, sessionId=22683, song=u'Ich mache einen Spiegel - Dream Part 4', status=200, ts=1538352001000, userAgent=u'\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId=u'1749042')", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#define necessary functions\ndef feature_engineering(df):\n    '''\n    Create necessary features to use machine learning algorithms.\n    First loads data set from file\n    \n    Resulting DF Strucutre:\n    \n    root\n     |-- userId: string\n     |-- downgraded: long\n     |-- cancelled: long\n     |-- visited_cancel: long\n     |-- visited_downgrade: long\n     |-- dailyHelpVisits: double\n     |-- dailyErrors: double\n     |-- free: integer\n     |-- paid: integer\n     |-- avgThumbsUp: double\n     |-- avgThumbsDOwn: double\n     |-- numFriends: long\n     |-- avgSongsTillHome: double\n     |-- avgTimeSkipped: double\n     |-- skipRate: double\n    \n    Inputs\n        filepath (str) - path to json dataset on file\n        \n    Outputs\n        data - engineered dataset\n    '''\n    df.persist() #maintain data in memory to speed up feature engineering process\n    #dataframe of user ids to merge onto\n    users = df.where((df.userId != \"\") | (df.sessionId != \"\"))\\\n        .select('userId').dropDuplicates()\n    df = df.where((df.userId != \"\") | (df.sessionId != \"\")) #clean dataframe\n    \n    #define custom functions\n    churn = udf(lambda x: int(x==\"Cancellation Confirmation\"), IntegerType())\n    downgrade_churn = udf(lambda x: int(x==\"Submit Downgrade\"), IntegerType())\n    visited_downgrade = udf(lambda x: int(x=='Downgrade'), IntegerType())\n    visited_cancel = udf(lambda x: int(x=='Cancel'), IntegerType())\n    song = udf(lambda x: int(x=='NextSong'), IntegerType())\n    days = lambda i: i * 86400 \n    get_day = udf(lambda x: datetime.datetime.fromtimestamp(x/1000), DateType())\n    skipped = udf(lambda x: int(x!=0), IntegerType())\n    free = udf(lambda x: int(x=='free'), IntegerType())\n    paid = udf(lambda x: int(x=='paid'), IntegerType())\n    home_visit=udf(lambda x : int(x=='Home'), IntegerType())\n    \n    #define windows\n    windowval = Window.partitionBy(\"userId\").orderBy(desc(\"ts\")).rangeBetween(Window.unboundedPreceding, 0)\n    session = Window.partitionBy(\"userId\", \"sessionId\").orderBy(desc(\"ts\"))\n    daywindow = Window.partitionBy('userId', 'date').orderBy(desc('ts'))\\\n        .rangeBetween(Window.unboundedPreceding, 0)\n\n    #count average daily occurences of thumbs up per user\n    avgThumbsUp = df.filter(df.page=='Thumbs Up')\\\n        .select('userId', 'page', 'ts')\\\n        .withColumn('date', get_day(col('ts')))\\\n        .groupBy('userId', 'date').agg({'page':'count'}).groupBy('userId')\\\n        .mean().withColumnRenamed('avg(count(page))', 'avgThumbsUp')\n\n    #count average daily occurences of thumbs up per user\n    avgThumbsDown = df.filter(df.page=='Thumbs Down')\\\n        .select('userId', 'page', 'ts')\\\n        .withColumn('date', get_day(col('ts')))\\\n        .groupBy('userId', 'date').agg({'page':'count'})\\\n        .groupBy('userId').mean()\\\n        .withColumnRenamed('avg(count(page))', 'avgThumbsDown')\n\n    #count the number of friends each user has\n    numFriends = df.filter(df.page=='Add Friend')\\\n        .select('userId', 'page')\\\n        .groupBy('userId').count().withColumnRenamed('count', 'numFriends')\n    \n    '''\n    Calculate average time of song skipped\n    process for calculating skipping variables\n\n    1. dont include thumbs up and down pages because that usually occurs \n        while playing and does not change song\n    2. create variable for if action is song\n    3. check if next action is song - this will check to see if someone is \n        skipping song or just leaving page\n    4. get the difference in timestamp for next action song playing\n    5. subtract the difference in timestamp from song length to see \n        how much of song was skipped\n    6. get descriptive stats\n    '''\n    skipping = df.select('userId', 'page', 'ts', 'length', 'sessionId', 'itemInSession')\\\n        .where((df.page != 'Thumbs Up') & (df.page != 'Thumbs Down'))\\\n        .withColumn('song', song('page')).orderBy('userId', 'sessionId', 'itemInSession')\\\n        .withColumn('nextActSong', lag(col('song')).over(session))\\\n        .withColumn('tsDiff', (lag('ts').over(session)-col('ts'))/1000)\\\n        .withColumn('timeSkipped', (floor('length')-col('tsDiff')))\\\n        .withColumn('roundedLength', floor('length'))\\\n        .where((col('song')==1) & ((col('nextActSong')!=0)&(col('timeSkipped')>=0)))\\\n        .withColumn('skipped', skipped('timeSkipped'))\\\n        .select('userId', 'timeSkipped', 'skipped', 'length', 'ts', 'tsDiff')\\\n        .groupBy('userId').agg({'skipped':'avg', 'timeSkipped':'avg'})\\\n        .withColumnRenamed('avg(skipped)', 'skipRate')\\\n        .withColumnRenamed('avg(timeSkipped)', 'avgTimeSkipped')\n    \n    #avg daily visits to help site\n    dailyHelpVisit = df.filter(df.page=='Help')\\\n        .select('userId', 'page', 'ts', 'length')\\\n        .withColumn('date', get_day(col('ts')))\\\n        .groupBy('userId', 'date').agg({'page':'count'})\\\n        .groupBy('userId').mean()\\\n         .withColumnRenamed('avg(count(page))', 'dailyHelpVisits')\n\n    #count average daily errors occured\n    dailyErrors = df.filter(df.page=='Error')\\\n        .select('userId', 'page', 'ts', 'length')\\\n        .withColumn('date', get_day(col('ts')))\\\n        .groupBy('userId', 'date').agg({'page':'count'})\\\n        .groupBy('userId').mean()\\\n        .withColumnRenamed('avg(count(page))', 'dailyErrors')\n    \n    #whether a user has downgraded\n    churn = df.withColumn(\"downgraded\", downgrade_churn(\"page\"))\\\n        .withColumn(\"cancelled\", churn(\"page\"))\\\n        .withColumn('visited_cancel', visited_cancel('page'))\\\n        .withColumn('visited_downgrade', visited_downgrade('page'))\\\n        .select(['userId', 'downgraded', 'cancelled', 'visited_cancel', 'visited_downgrade'])\\\n        .groupBy('userId').sum()\\\n        .withColumnRenamed('sum(downgraded)', 'downgraded')\\\n        .withColumnRenamed('sum(cancelled)', 'cancelled')\\\n        .withColumnRenamed('sum(visited_cancel)', 'visited_cancel')\\\n        .withColumnRenamed('sum(visited_downgrade)', 'visited_downgrade')\n\n    #assign the user a current level (free, paid) by dropping all duplicate values, and keeping most recent vals\n    user_level = df.select('userId', 'level','ts')\\\n        .where((df.level=='free')|(df.level=='paid'))\\\n        .orderBy(desc('ts')).drop('ts').dropDuplicates()\\\n        .withColumn('free', free('level'))\\\n        .withColumn('paid', paid('level')).drop('level')\n\n    #mark each song between home visit with a 1\n    cusum = df.filter((df.page == 'NextSong') | (df.page == 'Home')) \\\n        .select('userID', 'page', 'ts') \\\n        .withColumn('homevisit', home_visit(col('page'))) \\\n        .withColumn('songPeriod', Fsum('homevisit').over(windowval))\\\n    \n    #calculate average number of songs played between each home visit\n    avgSongsTillHome = cusum.filter((cusum.page=='NextSong'))\\\n        .groupBy('userId', 'songPeriod')\\\n        .agg({'songPeriod':'count'}).drop('songPeriod')\\\n        .groupby('userId').mean()\\\n        .withColumnRenamed('avg(count(songPeriod))', 'avgSongsTillHome')\n    \n    #combine user id on \n    df = users.join(churn, on='userId')\\\n        .join(dailyHelpVisit, on='userId')\\\n        .join(dailyErrors, on='userId')\\\n        .join(user_level, on='userId')\\\n        .join(avgThumbsUp, on='userId')\\\n        .join(avgThumbsDown, on='userId')\\\n        .join(numFriends, on='userId')\\\n        .join(avgSongsTillHome, on='userId')\\\n        .join(skipping, on='userId')\n    return df\n\ndef feature_scaling(df):\n    '''\n    Function takes care of scaling inputs into the model to between [0,1]\n    Otherwise if the values weren't scaled then the feature with highest values would dominate the training.\n    \n    Input\n        df (Spark DataFrame)\n        \n    Output\n        scaled_df (Spark DataFrame)\n    '''\n    df.persist() #keep df in memory to speed up computation\n    \n    feature_cols = df.drop('userId', 'cancelled').columns\n    assembler = VectorAssembler(inputCols=feature_cols,\\\n                                outputCol='feature_vec')\n    \n    #pyspark.ml expects target column to be names: 'labelCol', w/ type: Double\n    df = df.withColumn(\"label\", df[\"cancelled\"].cast(DoubleType()))\n    \n    #pyspark default name for features vector column: 'featuresCol'\n    minmaxscaler = MinMaxScaler(inputCol=\"feature_vec\", outputCol=\"features\")\n    \n    df = assembler.transform(df)\n    minmaxscaler_model = minmaxscaler.fit(df)\n    scaled_df = minmaxscaler_model.transform(df)\n    return scaled_df\n\ndef custom_evaluation(pred, model_name):\n    '''\n    Perform custom evaluation of predictions\n    \n    1.inspect with PySpark.ML evaluator (will use for pipeline)\n    2. use RDD-API; PySpark.MLLib to get metrics based on predictions \n    3. display confusion matrix\n    \n    Inspiration from: https://chih-ling-hsu.github.io/2018/09/17/spark-mllib\n    https://spark.apache.org/docs/2.2.0/mllib-evaluation-metrics.html\n    https://stackoverflow.com/questions/35572000/how-can-i-plot-a-confusion-matrix\n    \n    Inputs\n        preds(PySpark.ml.DataFrame) - predictions from model\n    '''\n    #want to evaluate binary class, auc_pr is best for imbalanced classes\n    tn_sum = pred.filter((pred.label == 0)&(pred.prediction==0)).count() #true negative\n    fn_sum = pred.filter((pred.label == 1)&(pred.prediction==0)).count() #false negative\n    fp_sum = pred.filter((pred.label == 0)&(pred.prediction==1)).count() #false positive\n    tp_sum = pred.filter((pred.label == 1)&(pred.prediction==1)).count() #true positive\n\n    print(\"{} \\n | tn:{}| fn:{}| fp:{}| tp:{}\".format(model_name, tn_sum, fn_sum, fp_sum, tp_sum))", "execution_count": 2, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "7dd4ff6bb97649469a368d29715a6878"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#prepare data for ML\ndf = feature_engineering(data)", "execution_count": 3, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "02d9cba539a648cb9438b075c1c191af"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#scale features for ML algo\ndf_scaled = feature_scaling(df)", "execution_count": 4, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b23d87777b06493db0066cfee300357a"}}, "metadata": {}}, {"output_type": "stream", "text": "----------------------------------------\nException happened during processing of request from ('127.0.0.1', 50604)\n----------------------------------------\nTraceback (most recent call last):\n  File \"/usr/lib64/python2.7/SocketServer.py\", line 290, in _handle_request_noblock\n    self.process_request(request, client_address)\n  File \"/usr/lib64/python2.7/SocketServer.py\", line 318, in process_request\n    self.finish_request(request, client_address)\n  File \"/usr/lib64/python2.7/SocketServer.py\", line 331, in finish_request\n    self.RequestHandlerClass(request, client_address, self)\n  File \"/usr/lib64/python2.7/SocketServer.py\", line 652, in __init__\n    self.handle()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 266, in handle\n    poll(authenticate_and_accum_updates)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 241, in poll\n    if func():\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 254, in authenticate_and_accum_updates\n    received_token = self.rfile.read(len(auth_token))\nTypeError: object of type 'NoneType' has no len()", "name": "stdout"}]}, {"metadata": {"scrolled": true, "trusted": true}, "cell_type": "code", "source": "df_scaled.collect()[0]", "execution_count": 5, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f43f80cb7ad3423a9d9d777b65e0e11b"}}, "metadata": {}}, {"output_type": "stream", "text": "Row(userId=u'1000280', downgraded=1, cancelled=1, visited_cancel=1, visited_downgrade=3, dailyHelpVisits=1.6, dailyErrors=1.0, free=1, paid=0, avgThumbsUp=3.533333333333333, avgThumbsDown=2.0625, numFriends=14, avgSongsTillHome=24.333333333333332, avgTimeSkipped=0.0, skipRate=0.0, label=1.0, feature_vec=DenseVector([1.0, 1.0, 3.0, 1.6, 1.0, 1.0, 0.0, 3.5333, 2.0625, 14.0, 24.3333, 0.0, 0.0]), features=DenseVector([0.1429, 1.0, 0.0226, 0.12, 0.0, 1.0, 0.0, 0.1378, 0.1678, 0.0588, 0.2301, 0.0, 0.0]))", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#split data for training\ntrain, rest = df_scaled.randomSplit([0.85, 0.15], seed=42)\nvalidation, test = rest.randomSplit([0.5,0.5], seed=42)", "execution_count": 6, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "a59d72a778f84c188a4bf1e7dbeb25a5"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "train.persist() #reduce computational time", "execution_count": 7, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "05dc47e25845415381217de27db1365a"}}, "metadata": {}}, {"output_type": "stream", "text": "DataFrame[userId: string, downgraded: bigint, cancelled: bigint, visited_cancel: bigint, visited_downgrade: bigint, dailyHelpVisits: double, dailyErrors: double, free: int, paid: int, avgThumbsUp: double, avgThumbsDown: double, numFriends: bigint, avgSongsTillHome: double, avgTimeSkipped: double, skipRate: double, label: double, feature_vec: vector, features: vector]", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "import time", "execution_count": 24, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "cfe32f8d56a74d69808fbcccdc1ed6ba"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#random forest classifier model\nfrom pyspark.ml.classification import RandomForestClassifier\nrf = RandomForestClassifier(numTrees=10)\nstart=time.time()\nrf_model = rf.fit(train)\nrf_preds = rf_model.transform(validation)\nend=time.time()\nprint(\"duration: \", end-start)\ncustom_evaluation(rf_preds, 'Random Forest')", "execution_count": 25, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f3e2e6ea977a4befac9373aced859633"}}, "metadata": {}}, {"output_type": "stream", "text": "('duration: ', 7.361753940582275)\nRandom Forest \n | tn:1043| fn:0| fp:0| tp:279", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#gradient boosted trees (ie ada boost)\nfrom pyspark.ml.classification import GBTClassifier\ngbtrees = GBTClassifier(maxIter=10)\nstart=time.time()\ngbtree_model = gbtrees.fit(train)\ngbtree_preds = gbtree_model.transform(validation)\nend=time.time()\nprint(\"duration: \", end-start)\ncustom_evaluation(gbtree_preds, 'Gradient Boosted Trees')", "execution_count": 26, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "0b7e5952e923417f89598f1bd379d45d"}}, "metadata": {}}, {"output_type": "stream", "text": "Exception in thread cell_monitor-26:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.6/threading.py\", line 864, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.6/site-packages/awseditorssparkmonitoringwidget-1.0-py3.6.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 178, in cell_monitor\n    job_binned_stages[job_id][stage_id] = all_stages[stage_id]\nKeyError: 4538\n\n", "name": "stderr"}, {"output_type": "stream", "text": "('duration: ', 52.26982617378235)\nGradient Boosted Trees \n | tn:1043| fn:0| fp:0| tp:279", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#SVM: https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-support-vector-machine\n\nfrom pyspark.ml.classification import LinearSVC\nsvm = LinearSVC(maxIter=10, regParam=0.1)\nstart=time.time()\nsvm_model=svm.fit(train)\nsvm_preds=svm_model.transform(validation)\nend=time.time()\nprint(\"duration: \", end-start)\ncustom_evaluation(svm_preds, 'Support Vector Machine')", "execution_count": 27, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "69d3b448ff654ed188035939754df487"}}, "metadata": {}}, {"output_type": "stream", "text": "('duration: ', 20.398164987564087)\nSupport Vector Machine \n | tn:1043| fn:0| fp:0| tp:279", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#logistic regression model\nfrom pyspark.ml.classification import LogisticRegression\nlogReg = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\nlrModel = logReg.fit(train)\nlr_preds = lrModel.transform(validation)\ncustom_evaluation(lr_preds, 'Logistic Regression')", "execution_count": 11, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "8ae031ce32394b708a68cd9828c1bee5"}}, "metadata": {}}, {"output_type": "stream", "text": "Logistic Regression \n | tn:1043| fn:279| fp:0| tp:0", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "#### Compare Results to make sure they are accurate"}, {"metadata": {"scrolled": true, "trusted": true}, "cell_type": "code", "source": "#visual check for predictions\nfor x in [svm_preds, lr_preds, gbtree_preds, rf_preds]:\n    x.select('features', 'rawPrediction', 'prediction', 'label').show(20)", "execution_count": 12, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "0765e3280517497e830ceddb7d3d0521"}}, "metadata": {}}, {"output_type": "stream", "text": "+--------------------+--------------------+----------+-----+\n|            features|       rawPrediction|prediction|label|\n+--------------------+--------------------+----------+-----+\n|[0.0,0.0,0.0,0.06...|[1.29454586908108...|       0.0|  0.0|\n|[0.0,0.0,0.0,0.1,...|[0.91650298266328...|       0.0|  0.0|\n|[0.0,0.0,0.120300...|[1.26981895598087...|       0.0|  0.0|\n|[0.0,0.0,0.022556...|[1.06079024054081...|       0.0|  0.0|\n|[0.0,0.0,0.120300...|[1.30126183663906...|       0.0|  0.0|\n|[0.14285714285714...|[1.66035588375266...|       0.0|  0.0|\n|[0.14285714285714...|[1.00878880062720...|       0.0|  0.0|\n|[0.0,0.0,0.037593...|[1.24246986138686...|       0.0|  0.0|\n|[0.14285714285714...|[1.52749153383729...|       0.0|  0.0|\n|[0.14285714285714...|[1.51957700662642...|       0.0|  0.0|\n|[0.14285714285714...|[1.31041568085742...|       0.0|  0.0|\n|[0.14285714285714...|[1.76545507949694...|       0.0|  0.0|\n|[0.0,0.0,0.052631...|[1.05925139204592...|       0.0|  0.0|\n|[0.28571428571428...|[1.49112193157191...|       0.0|  0.0|\n|[0.0,0.0,0.015037...|[0.88029793066967...|       0.0|  0.0|\n|[0.0,0.0,0.060150...|[1.21034612106780...|       0.0|  0.0|\n|[0.0,1.0,0.127819...|[-0.9208442402989...|       1.0|  1.0|\n|[0.28571428571428...|[-0.9351943662069...|       1.0|  1.0|\n|[0.14285714285714...|[-1.0389279243585...|       1.0|  1.0|\n|[0.0,0.0,0.165413...|[1.35456249742059...|       0.0|  0.0|\n+--------------------+--------------------+----------+-----+\nonly showing top 20 rows\n\n+--------------------+--------------------+----------+-----+\n|            features|       rawPrediction|prediction|label|\n+--------------------+--------------------+----------+-----+\n|[0.0,0.0,0.0,0.06...|[1.73692831608563...|       0.0|  0.0|\n|[0.0,0.0,0.0,0.1,...|[1.73692831608563...|       0.0|  0.0|\n|[0.0,0.0,0.120300...|[1.73692831608563...|       0.0|  0.0|\n|[0.0,0.0,0.022556...|[1.73692831608563...|       0.0|  0.0|\n|[0.0,0.0,0.120300...|[1.73692831608563...|       0.0|  0.0|\n|[0.14285714285714...|[1.73692831608563...|       0.0|  0.0|\n|[0.14285714285714...|[1.73692831608563...|       0.0|  0.0|\n|[0.0,0.0,0.037593...|[1.73692831608563...|       0.0|  0.0|\n|[0.14285714285714...|[1.73692831608563...|       0.0|  0.0|\n|[0.14285714285714...|[1.73692831608563...|       0.0|  0.0|\n|[0.14285714285714...|[1.73692831608563...|       0.0|  0.0|\n|[0.14285714285714...|[1.73692831608563...|       0.0|  0.0|\n|[0.0,0.0,0.052631...|[1.73692831608563...|       0.0|  0.0|\n|[0.28571428571428...|[1.73692831608563...|       0.0|  0.0|\n|[0.0,0.0,0.015037...|[1.73692831608563...|       0.0|  0.0|\n|[0.0,0.0,0.060150...|[1.73692831608563...|       0.0|  0.0|\n|[0.0,1.0,0.127819...|[0.10661600591133...|       0.0|  1.0|\n|[0.28571428571428...|[0.10661600591133...|       0.0|  1.0|\n|[0.14285714285714...|[0.10661600591133...|       0.0|  1.0|\n|[0.0,0.0,0.165413...|[1.73692831608563...|       0.0|  0.0|\n+--------------------+--------------------+----------+-----+\nonly showing top 20 rows\n\n+--------------------+--------------------+----------+-----+\n|            features|       rawPrediction|prediction|label|\n+--------------------+--------------------+----------+-----+\n|[0.0,0.0,0.0,0.06...|[1.32590267922033...|       0.0|  0.0|\n|[0.0,0.0,0.0,0.1,...|[1.32590267922033...|       0.0|  0.0|\n|[0.0,0.0,0.120300...|[1.32590267922033...|       0.0|  0.0|\n|[0.0,0.0,0.022556...|[1.32590267922033...|       0.0|  0.0|\n|[0.0,0.0,0.120300...|[1.32590267922033...|       0.0|  0.0|\n|[0.14285714285714...|[1.32590267922033...|       0.0|  0.0|\n|[0.14285714285714...|[1.32590267922033...|       0.0|  0.0|\n|[0.0,0.0,0.037593...|[1.32590267922033...|       0.0|  0.0|\n|[0.14285714285714...|[1.32590267922033...|       0.0|  0.0|\n|[0.14285714285714...|[1.32590267922033...|       0.0|  0.0|\n|[0.14285714285714...|[1.32590267922033...|       0.0|  0.0|\n|[0.14285714285714...|[1.32590267922033...|       0.0|  0.0|\n|[0.0,0.0,0.052631...|[1.32590267922033...|       0.0|  0.0|\n|[0.28571428571428...|[1.32590267922033...|       0.0|  0.0|\n|[0.0,0.0,0.015037...|[1.32590267922033...|       0.0|  0.0|\n|[0.0,0.0,0.060150...|[1.32590267922033...|       0.0|  0.0|\n|[0.0,1.0,0.127819...|[-1.3259026792203...|       1.0|  1.0|\n|[0.28571428571428...|[-1.3259026792203...|       1.0|  1.0|\n|[0.14285714285714...|[-1.3259026792203...|       1.0|  1.0|\n|[0.0,0.0,0.165413...|[1.32590267922033...|       0.0|  0.0|\n+--------------------+--------------------+----------+-----+\nonly showing top 20 rows\n\n+--------------------+--------------------+----------+-----+\n|            features|       rawPrediction|prediction|label|\n+--------------------+--------------------+----------+-----+\n|[0.0,0.0,0.0,0.06...|          [10.0,0.0]|       0.0|  0.0|\n|[0.0,0.0,0.0,0.1,...|          [10.0,0.0]|       0.0|  0.0|\n|[0.0,0.0,0.120300...|          [10.0,0.0]|       0.0|  0.0|\n|[0.0,0.0,0.022556...|          [10.0,0.0]|       0.0|  0.0|\n|[0.0,0.0,0.120300...|[9.28000000000000...|       0.0|  0.0|\n|[0.14285714285714...|          [10.0,0.0]|       0.0|  0.0|\n|[0.14285714285714...|          [10.0,0.0]|       0.0|  0.0|\n|[0.0,0.0,0.037593...|          [10.0,0.0]|       0.0|  0.0|\n|[0.14285714285714...|          [10.0,0.0]|       0.0|  0.0|\n|[0.14285714285714...|          [10.0,0.0]|       0.0|  0.0|\n|[0.14285714285714...|[9.74352527030425...|       0.0|  0.0|\n|[0.14285714285714...|          [10.0,0.0]|       0.0|  0.0|\n|[0.0,0.0,0.052631...|          [10.0,0.0]|       0.0|  0.0|\n|[0.28571428571428...|          [10.0,0.0]|       0.0|  0.0|\n|[0.0,0.0,0.015037...|          [10.0,0.0]|       0.0|  0.0|\n|[0.0,0.0,0.060150...|          [10.0,0.0]|       0.0|  0.0|\n|[0.0,1.0,0.127819...|          [0.0,10.0]|       1.0|  1.0|\n|[0.28571428571428...|          [0.0,10.0]|       1.0|  1.0|\n|[0.14285714285714...|          [0.0,10.0]|       1.0|  1.0|\n|[0.0,0.0,0.165413...|          [10.0,0.0]|       0.0|  0.0|\n+--------------------+--------------------+----------+-----+\nonly showing top 20 rows", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#PCA\n#https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.PCA\nfrom pyspark.ml.feature import PCA\n\n\n#first create PCA with all the feeatures kept- 13 features\npca_full = PCA(k=13, inputCol=\"features\", outputCol=\"pcaFeatures\")\npca_model_full = pca_full.fit(df_scaled)\npca_model_full.explainedVariance", "execution_count": 13, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "665cc5fb2d3a45eaa74b50ab6e91bf61"}}, "metadata": {}}, {"output_type": "stream", "text": "DenseVector([0.6684, 0.2304, 0.0372, 0.018, 0.0149, 0.01, 0.008, 0.0075, 0.0032, 0.0019, 0.0006, 0.0, 0.0])", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "The explained variance vector shows that 97.69% of the variance in the dataset can be explained by the first 6 features. We really do not need more features than that from PCA."}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "pca_final = PCA(k=6, inputCol=\"features\", outputCol=\"pcaFeatures\")\npca_model = pca_final.fit(df_scaled)\ndf_scaled_pca = pca_model.transform(df_scaled)", "execution_count": 14, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "7b53d39551af4b5991e5ca9417b1c335"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#split data for training\ntrain_pca, rest_pca = df_scaled_pca.randomSplit([0.85, 0.15], seed=42)\nvalidation_pca, test_pca = rest_pca.randomSplit([0.5,0.5], seed=42)", "execution_count": 15, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "fa080f3e732540c691afb49b560b5f85"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "rf_pca = RandomForestClassifier(featuresCol='pcaFeatures', numTrees=10)", "execution_count": 28, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "d5101fcff1874f02a9ca7a6f7dc8cc5f"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "start=time.time()\nrf_pca_model=rf_pca.fit(train_pca)\nrf_preds_pca=rf_pca_model.transform(validation_pca)\nend=time.time()\nprint(\"duration: \",end-start)\ncustom_evaluation(rf_preds_pca, 'Random Forest Classifier PCA(k=6)')", "execution_count": 29, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "dc05326a6e354f8abcc3fa0c1d262ca7"}}, "metadata": {}}, {"output_type": "stream", "text": "('duration: ', 9.36722993850708)\nRandom Forest Classifier PCA(k=6) \n | tn:1043| fn:0| fp:0| tp:279", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#test model on testing set\nrf_test_preds = rf_pca_model.transform(test_pca)\ncustom_evaluation(rf_test_preds, 'Random Forest Classifier Test')", "execution_count": 42, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "3d4ee0cc518c4a0a9867a611cff12fc7"}}, "metadata": {}}, {"output_type": "stream", "text": "Random Forest Classifier Test \n | tn:1144| fn:0| fp:0| tp:327", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "The test is fantastic!\n\nInvestigate parameters of final model"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "print(rf_pca_model.explainParams())", "execution_count": 40, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "56ddb35570f74820801a69fa913233dc"}}, "metadata": {}}, {"output_type": "stream", "text": "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. (default: False)\ncheckpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext (default: 10)\nfeatureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n]. (default: auto)\nfeaturesCol: features column name (default: features, current: pcaFeatures)\nimpurity: Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini)\nlabelCol: label column name (default: label)\nmaxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\nmaxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\nmaxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. (default: 256)\nminInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\nminInstancesPerNode: Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\nnumTrees: Number of trees to train (>= 1) (default: 20, current: 10)\npredictionCol: prediction column name (default: prediction)\nprobabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities (default: probability)\nrawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)\nseed: random seed (default: -4140900678877021401)\nsubsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\nthresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold (undefined)", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "import pprint\npp = pprint.PrettyPrinter(indent=0)\npp.pprint(rf_pca_model.extractParamMap())", "execution_count": 41, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "bf8803099fb341f7a15744a540265917"}}, "metadata": {}}, {"output_type": "stream", "text": "{Param(parent=u'RandomForestClassifier_690037145a18', name='featuresCol', doc='features column name'): 'pcaFeatures',\nParam(parent=u'RandomForestClassifier_690037145a18', name='featureSubsetStrategy', doc='The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n].'): 'auto',\nParam(parent=u'RandomForestClassifier_690037145a18', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees.'): False,\nParam(parent=u'RandomForestClassifier_690037145a18', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext'): 10,\nParam(parent=u'RandomForestClassifier_690037145a18', name='seed', doc='random seed'): -4140900678877021401,\nParam(parent=u'RandomForestClassifier_690037145a18', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation.'): 256,\nParam(parent=u'RandomForestClassifier_690037145a18', name='numTrees', doc='Number of trees to train (>= 1)'): 10,\nParam(parent=u'RandomForestClassifier_690037145a18', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities'): 'probability',\nParam(parent=u'RandomForestClassifier_690037145a18', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0,\nParam(parent=u'RandomForestClassifier_690037145a18', name='predictionCol', doc='prediction column name'): 'prediction',\nParam(parent=u'RandomForestClassifier_690037145a18', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1,\nParam(parent=u'RandomForestClassifier_690037145a18', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name'): 'rawPrediction',\nParam(parent=u'RandomForestClassifier_690037145a18', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini',\nParam(parent=u'RandomForestClassifier_690037145a18', name='labelCol', doc='label column name'): 'label',\nParam(parent=u'RandomForestClassifier_690037145a18', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 32,\nParam(parent=u'RandomForestClassifier_690037145a18', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5,\nParam(parent=u'RandomForestClassifier_690037145a18', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0}", "name": "stdout"}]}], "metadata": {"kernelspec": {"name": "pysparkkernel", "display_name": "PySpark", "language": ""}, "language_info": {"name": "pyspark", "mimetype": "text/x-python", "codemirror_mode": {"name": "python", "version": 2}, "pygments_lexer": "python2"}}, "nbformat": 4, "nbformat_minor": 2}