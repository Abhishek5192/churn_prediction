# Predicting Churn - Apache Spark
*Predicting Churn from user's activity data*

## Dependencies
- Python 3.6.3
- PySpark 2.3.2
- Pandas 0.20.3

## Motivation
I am always eager to learn new frameworks and expand my capabilities, so when I heard about the possibility of a project utilized Apache Spark and Hadoop I was already very intrigued. Having learned the basics of Apache Spark's PySpark API, there is no better way of displaying machine learning prowess than in a big data context. This project evolves around a key business issue that many firms face; *How can we know which customers want to leave, and how can our marketing department target them?* 
Business applications are what excites me the most about Data Science. Proving that I can glean valuable insights from corporate-sized data sources would prove to me that i can say **Big Data** as more than just a buzz word.

## Repository
   
    .
    ├── 
    │   ├── Sparkify.ipynb            # initial development
    │   |
    │   └── market_cap_fill.csv    # CSV file to fill Null market capitalisation data
    │  
    └── ...
## Summary

## Acknowledgements
 - (PySpark Window Functions)[https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html]
 - Udacity DSND Apache Spark Lesson
 - 

Reqs:
Student must have a Github repository of their project. The repository must have a README.md file that communicates the libraries used, the motivation for the project, the files in the repository with a small description of each, a summary of the results of the analysis, and necessary acknowledgements. If the student submits a web app rather than a blog post, then the Project Definition, Analysis, and Conclusion should be included in the README file, or in their Jupyter Notebook. Students should not use another student's code to complete the project, but they may use other references on the web including StackOverflow and Kaggle to complete the project.



